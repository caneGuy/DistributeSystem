```
对于spark的一些记录和思考
```
#### Why are RDDs called immutable if they allow for transformations?
```
A transformation produces a new RDD, but it may not be materialized
explicitly, because transformations are computed lazily. In the
pagerank example, each loop iteration produces a new distrib and rank
RDD, as shown in the corresponding lineage graph.
```
#### How do applications figure out the location of an RDD?
```
The application names RDDs with variable names in Scala. Each RDD
has location information associated with it, in its metadata (see
Table 3). The scheduler uses all this information to colocate
computations with the data. An RDD may be generated by multiple nodes
but it is for different partitions of the RDD.
```
#### How does Spark achieve fault tolerance?
```
When persisting an RDD, the programmer can specify that it must be
replicated on a few machines. Spark doesn't need complicated protocols
like Raft, however, because RDDs are immutable and can always be
recomputed using the lineage graph.
cache和checkpoint可以优化性能
```
#### What applications can Spark support well that MapReduce/Hadoop cannot support?
```
sql,streaming作业.
对于batch作业的话，只是有些性能上的问题，到没有说是绝对不能实现。
```